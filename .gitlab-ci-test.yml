workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'

variables:
  ENV: "test"
  DOCKER_CONFIG: "$CI_PROJECT_DIR/.docker"
  EKS_CLUSTER_NAME: "eks-k8sgptxlocalai-stage-env"
  AWS_REGION: "eu-west-3"
  NODE_COUNT: "2"
  ACCOUNT_ID: "$AWS_ACCOUNT_ID"
  DOCKER_IMAGE: "app_temoin"
  APP_INGRESS_NAME: ingress-stock-app
  APP_VERSION: "1.0.8"
  OPENAI_TOKEN: "$OPENAI_TOKEN"
  TEST_ENV_CIDR: "172.16.0.0/16"

stages:
  - build
  - deploy
  - provision
  - monitor
  - infra_test
  - destroy_infra
  - destroy_cluster

build-docker-image:
  stage: build
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image: 
    name: docker:24.0.5
  services:
    - docker:dind
  variables: 
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: ""
  before_script:
    # Connection to the gitlab Registry
    - echo "$PULL_PAT_TOKEN" | docker login -u "$CI_REGISTRY_USER" $CI_REGISTRY --password-stdin
    # Create a separate docker network for testing
    - docker network create test_env
  script:
    - echo "Pass"
    # Build a first test image
    # Unit tests are integrated into the Dockerfile build
    - docker build --target build -t $CI_REGISTRY/$DOCKER_IMAGE:test ./app
    # Run the container on the test network to copy the report.xml - Result report of the unit tests
    - docker run -d --name test_app_cont --hostname stock_analysis --network test_env -p 5000:5000 $CI_REGISTRY/$DOCKER_IMAGE:test
    # Wait for container to spin-up before attempting to copy the targeted content
    - sleep 5
    - docker cp test_app_cont:/app/report.xml $CI_PROJECT_DIR/report.xml
    # Remove useless container
    - docker rm -f test_app_cont
    # Build the release candidate image for Owasp testing
    - docker build --target production -t $CI_REGISTRY/nevii/leverage-your-own-k8s-expert/$DOCKER_IMAGE:$APP_VERSION ./app
    # Run the release candidate image for Owasp testing
    - docker run -d --name test_app_cont --hostname stock_analysis --network test_env -p 5000:5000 $CI_REGISTRY/nevii/leverage-your-own-k8s-expert/$DOCKER_IMAGE:$APP_VERSION
    # Make sure the rc container is up and running as expected
    - until docker run --network test_env --rm curlimages/curl curl http://stock_analysis:5000; do echo "Waiting for app to start..."; sleep 5; done
    # logout to pull image from public docker registry
    - docker logout
    # Run the owasp container with the automation framework symlinked to the stock_analysis_zap.yml template in repo
    - docker run --rm --name owaspzap --network test_env -v $(pwd):/zap/reports/:rw -v $(pwd):/zap/wrk/:rw -t zaproxy/zap-stable bash -c "zap.sh -cmd -addonupdate; zap.sh -cmd -quickurl http://stock_analysis:5000 -autorun /zap/wrk/stock_analysis_zap.yml"
    # Login again to our Gitlab container registry to push the rc image there
    - echo "$PULL_PAT_TOKEN" | docker login -u "$CI_REGISTRY_USER" $CI_REGISTRY --password-stdin
    - docker push $CI_REGISTRY/nevii/leverage-your-own-k8s-expert/$DOCKER_IMAGE:$APP_VERSION
  tags:
    - build_job_docker
  artifacts:
    # Export the output as an artifact with 30 days retention, only viewable by dev on the project
    untracked: false
    when: on_success
    access: developer
    expire_in: 30 days
    name: "build_and_reports"
    paths:
      - ./app
      - combinedHtmlReport.html
      - report.xml
      - .docker/config.json
  # only:
  #   - test
  #   - features1

deploy_eks:
  stage: deploy
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.3
  environment:
    name: pre-production
  script:
    # Set your AWS credentials for aws cli login
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    # Create EKS cluster
    - eksctl create cluster --name $EKS_CLUSTER_NAME --version 1.29 --vpc-cidr $TEST_ENV_CIDR --node-type t2.xlarge --nodes $NODE_COUNT --tags "Environment=Test" --alb-ingress-access --node-private-networking --vpc-nat-mode Single || true
    # Recreate the Load Balancer policy if needed
    # # - curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
    # # - aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json
    # Update current client to use the API of or EKS cluster
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    # Create and associate an iam oidc identity provider. Enable managed identity for nodes and pods, needed for plugin deployment 
    - eksctl utils associate-iam-oidc-provider --cluster=$EKS_CLUSTER_NAME --approve || true
    # Create access entry, authentication mode for Kubernetes API endpoint cluster authentication. Define the service principal to use
    - aws eks create-access-entry --cluster-name $EKS_CLUSTER_NAME --principal-arn arn:aws:iam::$ACCOUNT_ID:root --type STANDARD --username admin_eks || true
    # Create the access policy for EKS cluster so it is authorized to interact with AWS resources
    - aws eks associate-access-policy --cluster-name $EKS_CLUSTER_NAME  --principal-arn arn:aws:iam::$ACCOUNT_ID:root --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy --access-scope type=cluster || true
    # Create the service account for managing the ALB through kubernetes objects
    - eksctl create iamserviceaccount --cluster=$EKS_CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller --role-name AmazonEKSLoadBalancerControllerRole --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve
    # Create CNI addon, necessary for the aws load balancer controller
    - eksctl create addon --cluster $EKS_CLUSTER_NAME --name vpc-cni --version latest --force
    # Create EBS csi driver addon, need to provision a managed EBS volume for deploying the LLM
    - eksctl create addon --name aws-ebs-csi-driver --cluster $EKS_CLUSTER_NAME --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --force
    # Add & Install load balancer controller with Helm through eks repo, using previously created service account
    - helm repo add eks https://aws.github.io/eks-charts
    - helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$EKS_CLUSTER_NAME --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller || true
    - sleep 10 # aws-load-balancer-webhook-service needs time to be up and running
  tags:
    # Target the docker gitlab instance
    - build_job_docker
  needs:
    # Import artefact form build job
    - job: build-docker-image
      artifacts: true
# # # #   # when: manual
  # only:
  #   - test
  #   - features1

deploy_db:
  stage: deploy
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.3
  environment:
    name: pre-production
  before_script:
    # yq packages needs to be installed on the runner instance
    - wget https://github.com/mikefarah/yq/releases/download/v4.25.1/yq_linux_amd64 -O /usr/local/bin/yq
    - chmod +x /usr/local/bin/yq
    - export TF_PASSWORD=${GITLAB_ACCESS_TOKEN}
    # Switch working directory to tf files locations
    - cd $CI_PROJECT_DIR/Terraform_Infra
    - rm -rf .terraform
    - terraform --version
    # Authenticate to Gitlab State management
    - terraform init -backend-config=password=${TF_PASSWORD}
    # Get the eks cluster vpc id and the route table
    # The vpc is needed to deploy the RDS db and the route table to make sure the cluster subnets and the database subnet can communicate
    - VPC_ID=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --region $AWS_REGION --query "cluster.resourcesVpcConfig.vpcId" --output text)
    - RTB_ID=$(aws ec2 describe-route-tables --region $AWS_REGION --filters "Name=vpc-id,Values=$VPC_ID" --query "RouteTables[?Associations[?Main==\`true\`]].RouteTableId" --output text)
  script:
    - terraform fmt -recursive
    - terraform validate
    # Supply needed secrets from Gitlab var repository
    # Deploy the RDS instance and update the network configuration with terraform
    - terraform plan -var "vpc_id=$VPC_ID" -var "rtb_id=$RTB_ID" -var "db_username=$RDS_USERNAME" -var "db_password=$RDS_PASSWORD" -var "environment=$ENV"
    - terraform apply -var "vpc_id=$VPC_ID" -var "rtb_id=$RTB_ID" -var "db_username=$RDS_USERNAME" -var "db_password=$RDS_PASSWORD" -var "environment=$ENV" --auto-approve
    # Convert the output into yaml to be usable easier by k8s
    - terraform output -json | yq eval -P '.' > vars.yml
    - cat vars.yml
  needs:
    # Depends on EKS job
    - deploy_eks
    # Export the output as an artifact with 1 day retention, only viewable by dev on the project
  artifacts:
    untracked: false
    when: on_success
    access: developer
    expire_in: 1 days
    paths:
      - $CI_PROJECT_DIR/Terraform_Infra/vars.yml
    # Target the docker gitlab instance
  tags:
    - build_job_docker
# #   # when: manual
  # only:
  #   - test
  #   - features1

  
hydrate_db:
  stage: provision
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  # image: python:3.9.17-slim-bullseye
  before_script:
    # Export credentials (optional if the dev pipeline has been launched before)
    - export AWS_PROFILE=aws_profile
    - export AWS_REGION=$AWS_REGION
    # Export ansible.cfg as Ansible config file
    - export ANSIBLE_CONFIG=ansible.cfg
  script:
    # Make sure you are authentified to AWS 
    - aws sts get-caller-identity
    - ansible --version
    # verify that the target in inventory is reachable
    - ansible -m ping all
    - ansible-inventory --list
    - ansible-inventory --graph
    # Execute the playbook Ansible, use the artifacts from previous job to successfully log into the database
    - ansible-playbook -i Ansible/inventory.ini Ansible/playbook.yml -e "@$CI_PROJECT_DIR/Terraform_Infra/vars.yml"
  needs:
    # Depends on the RDS deploy job and it's artifact for identifying to the database
    - job: deploy_db
      artifacts: true
# #   # when: manual
  # only:
  #   - test
  #   - features1

deploy_stock_app:
  stage: provision
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.3
  before_script:
    # Install the necessary packages to use the exported yaml artifacts values in the files
    - pip install pyyaml
    - wget https://github.com/mikefarah/yq/releases/download/v4.25.1/yq_linux_amd64 -O /usr/local/bin/yq
    - chmod +x /usr/local/bin/yq
  script:
    # Set the cluster endpoint as target for kubectl commands
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    # Use the artifact from build job to authenticate to the Gitlab container registry and pull the docker image built
    # The extracted values will be used to create a kubernetes secret
    - |
      cat <<EOF > gitlab_secret.yml
      apiVersion: v1
      kind: Secret
      metadata:
        name: gitlab-cred
        namespace: default
      data:
        .dockerconfigjson: $(cat .docker/config.json | base64 -w 0)
      type: kubernetes.io/dockerconfigjson
      EOF
    - kubectl apply -f gitlab_secret.yml --dry-run=client -o yaml | kubectl apply -f -
    # Create a service in the cluster for communicating with the RDS instance
    # The rds dns adress extracted form the deploy db job will be used
    - |
      cat <<EOF > mysql-service.yml
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: mysql-service
        name: mysql-service
      spec:
        externalName: $(yq eval '.rds_address.value' $CI_PROJECT_DIR/Terraform_Infra/vars.yml)
        selector:
          app: mysql-service
        type: ExternalName
      status:
        loadBalancer: {}
      EOF
    # Create a secret to be used for the app authentication toward the RDS instance
    # The values used will be extracted from the deploy db job
    - kubectl apply -f mysql-service.yml --dry-run=client -o yaml | kubectl apply -f -
    - |
      cat <<EOF > db_secret.yml
      apiVersion: v1
      kind: Secret
      metadata:
        name: db-credentials
        namespace: default
      data:
        DB_USER: $(yq eval '.database_username.value' $CI_PROJECT_DIR/Terraform_Infra/vars.yml | base64 -w 0)
        DB_PASSWORD: $(yq eval '.database_password.value' $CI_PROJECT_DIR/Terraform_Infra/vars.yml | base64 -w 0)
        RDS_ADDRESS: $(yq eval '.rds_address.value' $CI_PROJECT_DIR/Terraform_Infra/vars.yml | base64 -w 0)
        DB_NAME: $(yq eval '.database_name.value' $CI_PROJECT_DIR/Terraform_Infra/vars.yml | base64 -w 0)
      EOF
    - kubectl apply -f db_secret.yml --dry-run=client -o yaml | kubectl apply -f -
    # Configure a storage class (EBS) for storage provisioning from k8s
    - kubectl apply -f $CI_PROJECT_DIR/K8s/manifests/stock_analysis_app/storageclass.yml --dry-run=client -o yaml | kubectl apply -f -
    # Configure persistent volume claim to deploy EBS volumes for persistent datas
    - kubectl apply -f $CI_PROJECT_DIR/K8s/manifests/stock_analysis_app/pvc.yml --dry-run=client -o yaml | kubectl apply -f -
    # Deploy the application using a deployment manifest. A service and an Ingress are also defined
    - kubectl apply -f $CI_PROJECT_DIR/K8s/manifests/stock_analysis_app/deployment_stock.yml --dry-run=client -o yaml | kubectl apply -f -
    # Make sure the dns name is associated and the LB is up and running before extracting the dns adress into as environment variable into a file
    # Loop exits when non zero value is returned, retries every 10 secondes
    - |
      while [ -z "$LB_DNS" ]; do 
        echo "Waiting for Ingress to apply and return Load Balancer DNS name, retrying in 5 seconds..."
        sleep 5
        LB_DNS=$(kubectl get ingress ingress-stock-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
      done
    # Extract and store the ALB DNS adress value
    - LB_DNS=$(kubectl get ingress ingress-stock-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
    - echo "LB_DNS=$LB_DNS" > lb_dns.env
    # Make sure the key:value is correclty registered
    - cat lb_dns.env
  needs:
    # Job dependency, with artifact import enabled from them
    - job: deploy_db
      artifacts: true
    - job: build-docker-image
      artifacts: true
    # Export the output as an artifact with 1 day retention, only viewable by dev on the project
  artifacts:
    untracked: false
    when: on_success
    access: developer
    expire_in: 1 days
    paths:
      - $CI_PROJECT_DIR/lb_dns.env
  # Target the docker gitlab instance
  tags:
    - build_job_docker
  # when: manual
  # only:
  #   - test
  #   - features1

monitoring:
  stage: monitor
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.3
  script:
    # Make sur you are authentified to aws account with access to EKS endpoint
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    # Set the EKS cluster endpoint as target for kubectl commands
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    - helm list --all-namespaces
    # - helm uninstall kube-prometheus-stack prometheus-community/kube-prometheus-stack -n monitor-graf-prom || true
    # Add & Install the PrometheusxGrafana stack with Helm
    - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts || true
    - helm repo update
    - helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n monitor-graf-prom --create-namespace -f $CI_PROJECT_DIR/K8s/manifests/monitoring/values.yml || true
    # - helm upgrade kube-prometheus-stack prometheus-community/kube-prometheus-stack -n monitor-graf-prom -f $CI_PROJECT_DIR/K8s/manifests/monitoring/values.yml --reuse-values || true
    - kubectl apply -f $CI_PROJECT_DIR/K8s/manifests/monitoring/servicemonitor_app.yml --dry-run=client -o yaml | kubectl apply -f -
    # Target the docker gitlab instance
  tags:
    - build_job_docker
  needs:
    # Job dependency
    - deploy_eks
    - deploy_stock_app
#   # when: manual
  # only:
  #   - test
  #   - features1

# Job to run the load test using the DNS fetched in the previous job
locust_loadtest:
  stage: infra_test
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image: python:3.9-slim
  before_script: 
    # Add & Install locust packages from the python library
    - pip install locust
  script:
    # Wait for the load balancer to be up and running
    - sleep 120
    # Use the .env file from deploy app job as environment variable source
    - source lb_dns.env
    # Launch locust load test against extracted alb dns name value - 100 users simulated for 5 minutes requesting the index of the app
    - locust -f $CI_PROJECT_DIR/Locust/locustfile.py --headless --users 100 --spawn-rate 10 --run-time 5m --host=http://$LB_DNS
  needs:
    # Job dependency, with artifact import enabled from them
    - job: deploy_stock_app
      artifacts: true
    - job: monitoring
      artifacts: false
  # Target the docker gitlab instance
  tags:
    - build_job_docker
  # Runs only for the test branch
  # only:
  #   - test
  #   - features1
  # when: manual

deploy_ai_analyzer:
  stage: infra_test
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.3
  script:
    # Make sur you are authentified to aws account with access to EKS endpoint
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    # Set the EKS cluster endpoint as target for kubectl commands
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    # Add & install k8sgpt packages with Hekm
    - helm repo add k8sgpt https://charts.k8sgpt.ai/ || true
    - helm repo update
    # Verify adding of k8sgpt packages
    - helm list --all-namespaces
    # # For maintenance purposes, crd of k8sgpt need to be removed before attempting to update
    - kubectl delete crd k8sgpts.core.k8sgpt.ai || true
    - helm uninstall release --namespace k8sgpt-operator-system || true
    # Install k8sgpt package release
    - helm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace || true
    # Create secret from openAI token value
    - kubectl create secret generic k8sgpt-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n k8sgpt-operator-system || true
    # # Create the cr for the k8sgpt operator
    # # Results of cluster analysis will be available as results resources in the operator namespace
    - kubectl apply -f $CI_PROJECT_DIR/K8s/manifests/k8sgpt_cluster_operator/k8sgpt_operator.yml --dry-run=client -o yaml | kubectl apply -f -
    # This loop extracts results from k8sgpt Analyzer 
    - | 
      while true; do
        # Try to run the command and register the error output in stdout if there is one 
        OUTPUT=$(kubectl get results -n k8sgpt-operator-system -o json | jq . 2>&1)

        # Check if the command was successful by searching for the "openai" string in the command output
        if echo "$OUTPUT" | grep -q "openai"; then
          # If there openai output (specific to the results here), loop ends
          break
        fi

        # If there is an error, print a message and retry after a set delay
        echo "Resource not available yet, retrying in 5 seconds..."
        sleep 5
      done
    - echo "$OUTPUT" > k8sresults.json
    # Delete the deployment by deleting the crd when results resources are created
    # OpenAi API does cost around 7e-5 
    # K8sgpt analyses the cluster non stop otherwise
    - kubectl delete crd k8sgpts.core.k8sgpt.ai
  # Target the docker gitlab instance
    - helm uninstall release --namespace k8sgpt-operator-system || true
  tags:
    - build_job_docker
  artifacts:
    untracked: false
    when: on_success
    access: developer
    expire_in: 30 days
    paths:
      - $CI_PROJECT_DIR/k8sresults.json
  needs:
  # Job dependency
    - deploy_eks
    - monitoring
  # Runs only for the test branch
  # only:
  #   - test
  #   - features1
#   # when: manual

destroy:
  stage: destroy_infra
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.3
  before_script:
    - export TF_PASSWORD=${GITLAB_ACCESS_TOKEN}
    - VPC_ID=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --region $AWS_REGION --query "cluster.resourcesVpcConfig.vpcId" --output text)
    - RTB_ID=$(aws ec2 describe-route-tables --region $AWS_REGION --filters "Name=vpc-id,Values=$VPC_ID" --query "RouteTables[?Associations[?Main==\`true\`]].RouteTableId" --output text)
    - cd $CI_PROJECT_DIR/Terraform_Infra
    - terraform init -backend-config=password=${TF_PASSWORD}
  script:
    - terraform destroy -var "vpc_id=$VPC_ID" -var "rtb_id=$RTB_ID" -var "db_username=$RDS_USERNAME" -var "db_password=$RDS_PASSWORD" -var "environment=$ENV" --auto-approve
  when: manual
  tags:
    - build_job_docker

clean:
  stage: destroy_cluster
  rules: 
    - if: $CI_COMMIT_BRANCH == 'test'
    # - if: $CI_PIPELINE_SOURCE == 'parent_pipeline'
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.3
  script:
    - echo $EKS_CLUSTER_NAME
    - echo $AWS_REGION
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME  # $EKS_CLUSTER_NAME demo-eks-appv1
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    - eksctl delete cluster --name $EKS_CLUSTER_NAME
  tags:
    - build_job_docker
  when: manual