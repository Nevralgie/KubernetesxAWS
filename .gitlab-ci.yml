variables:
  DOCKER_CONFIG: "$CI_PROJECT_DIR/.docker"
  EKS_CLUSTER_NAME: "demo-eks-appv1"
  AWS_REGION: "eu-west-3"
  NODE_COUNT: "3"
  ACCOUNT_ID: "577409777403"


stages:
  - build
  - prepare
  - validate
  # - plan
  - apply
  - deploy
  # - provision
  - monitor
  - destroy

# plan:
#   stage: plan
#   image:
#     name: hashicorp/terraform:latest
#     entrypoint:
#       - '/usr/bin/env'
#       - 'PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'
#   before_script:
#     - export AWS_ACCESS_KEY=${AWS_ACCESS_KEY_ID}
#     - export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
#     - rm -rf .terraform
#     - terraform --version
#     - terraform init
#   script:
#     - terraform plan
#   dependencies:
#     - validate

deploy:
  stage: deploy
  image:
    name: amazon/aws-cli
    entrypoint: [""]
  environment:
    name: production
  before_script:
    - whoami
    - ls
    - yum update -y
    - yum install -y openssl git tar gzip build-essential # needed for Helm chart verification
  script:
    - curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
    - mv /tmp/eksctl /usr/local/bin
    - eksctl version
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
    - curl -L https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 -o /tmp/get_helm.sh
    - chmod 0700 /tmp/get_helm.sh
    - /tmp/get_helm.sh # Install Helm
    - echo $EKS_CLUSTER_NAME
    - echo $AWS_REGION
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    - eksctl create cluster --name $EKS_CLUSTER_NAME --region $AWS_REGION --version 1.29 --alb-ingress-access --nodes $NODE_COUNT
    # - curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
    # - aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json
    # - aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
    - eksctl utils associate-iam-oidc-provider --region=$AWS_REGION --cluster=$EKS_CLUSTER_NAME --approve
    - aws eks create-access-entry --cluster-name $EKS_CLUSTER_NAME --principal-arn arn:aws:iam::$ACCOUNT_ID:root --type STANDARD --username admin_eks
    - aws eks associate-access-policy --cluster-name $EKS_CLUSTER_NAME  --principal-arn arn:aws:iam::$ACCOUNT_ID:root --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy --access-scope type=cluster
    - eksctl create iamserviceaccount --cluster=$EKS_CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller --role-name AmazonEKSLoadBalancerControllerRole --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve
    # - aws eks describe-addon --cluster-name $EKS_CLUSTER_NAME --addon-name vpc-cni --query addon.addonVersion --output text
    - eksctl create addon --cluster $EKS_CLUSTER_NAME --name vpc-cni --version latest --force # Necessary for aws load balancer controller
    - helm repo add eks https://aws.github.io/eks-charts
    - helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$EKS_CLUSTER_NAME --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller
    - sleep 30 # aws-load-balancer-webhook-service needs time to be up and running, or else next line will fail
    - kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/examples/2048/2048_full.yaml
  tags:
    - build_job_docker
  when: manual

monitoring:
  stage: monitor
  image:
    name: amazon/aws-cli
    entrypoint: [""]
  before_script: 
    - yum update -y
    - yum install -y openssl git tar gzip build-essential
  script:
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
    - echo $EKS_CLUSTER_NAME
    - echo $AWS_REGION
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    - curl -L https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 -o /tmp/get_helm.sh
    - chmod 0700 /tmp/get_helm.sh
    - /tmp/get_helm.sh
    - helm repo add eks https://aws.github.io/eks-charts
    - helm repo update eks
    - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    - helm repo update
    - |
      cat <<EOF > values.yaml
      prometheus:
        prometheusSpec:
          serviceMonitorSelectorNilUsesHelmValues: false
          ruleSelectorNilUsesHelmValues: false
          podMonitorSelectorNilUsesHelmValues: false
          probeSelectorNilUsesHelmValues: false
      grafana:
        enabled: true
        adminPassword: Azurev69007
        service:
          type: ClusterIP
          port: 80
          targetPort: 3000
        ingress:
          enabled: true
          ingressClassName: alb
          annotations:
            alb.ingress.kubernetes.io/scheme: internet-facing
            alb.ingress.kubernetes.io/target-type: ip
          paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: kube-prometheus-stack-grafana
                  port:
                    number: 80
          tls: []
      EOF
    # - helm uninstall kube-prometheus-stack
    - helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -f values.yaml
  tags:
    - build_job_docker
  when: manual

clean:
  stage: destroy
  image:
    name: amazon/aws-cli
    entrypoint: [""]
  environment:
    name: production
  before_script:
    - whoami
    - ls
    - yum update -y
    - yum install -y openssl git tar gzip build-essential # needed for Helm chart verification
  script:
    - curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
    - mv /tmp/eksctl /usr/local/bin
    - eksctl version
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
    - curl -L https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 -o /tmp/get_helm.sh
    - chmod 0700 /tmp/get_helm.sh
    - /tmp/get_helm.sh # Install Helm
    - echo $EKS_CLUSTER_NAME
    - echo $AWS_REGION
    - aws eks update-kubeconfig --name demo-eks-appv1 # $EKS_CLUSTER_NAME
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    # - kubectl delete -f 2048_full.yaml
    # - eksctl delete fargateprofile --cluster $EKS_CLUSTER_NAME --name <fargate-profile-name>
    # - eksctl delete nodegroup --cluster $EKS_CLUSTER_NAME --name <node-group-name>
    - eksctl delete cluster --name $EKS_CLUSTER_NAME
  tags:
    - build_job_docker
  when: manual
