variables:
  DOCKER_CONFIG: "$CI_PROJECT_DIR/.docker"
  EKS_CLUSTER_NAME: "demo-eks-k8sgptxlocalai-v1"
  AWS_REGION: "eu-west-3"
  NODE_COUNT: "2"
  ACCOUNT_ID: "577409777403"


stages:
  - build
  - prepare
  - validate
  # - plan
  - apply
  - deploy
  # - provision
  - ai
  - monitor
  - destroy

build-docker-image:
  stage: build
  image: docker:24.0.5
  services:
    - docker:dind
  variables:
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: "" # Vide pour l'instant pour établie la connection entre les conteneurs
  before_script:
    - echo "$PULL_PAT_TOKEN" | docker login -u "$CI_REGISTRY_USER" $CI_REGISTRY --password-stdin # Connection au registry gitlab
  script:
    - echo "Yé boiiiiii"
    - docker build -t $CI_REGISTRY/nevii/terraform_eks/awsclitoolbox:v0.2 .
    - docker push $CI_REGISTRY/nevii/terraform_eks/awsclitoolbox:v0.2
  artifacts:
    paths:
      - .docker/config.json
  tags:
  - build_job_docker
  when: manual


deploy:
  stage: deploy
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.2
  environment:
    name: production
  script:
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    - eksctl create cluster --name $EKS_CLUSTER_NAME --version 1.29 --nodes $NODE_COUNT --alb-ingress-access || true
    # - curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
    # - aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    - eksctl utils associate-iam-oidc-provider --cluster=$EKS_CLUSTER_NAME --approve || true
    - aws eks create-access-entry --cluster-name $EKS_CLUSTER_NAME --principal-arn arn:aws:iam::$ACCOUNT_ID:root --type STANDARD --username admin_eks || true
    - aws eks associate-access-policy --cluster-name $EKS_CLUSTER_NAME  --principal-arn arn:aws:iam::$ACCOUNT_ID:root --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy --access-scope type=cluster || true
    - eksctl create iamserviceaccount --cluster=$EKS_CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller --role-name AmazonEKSLoadBalancerControllerRole --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve
    # - aws eks describe-addon --cluster-name $EKS_CLUSTER_NAME --addon-name vpc-cni --query addon.addonVersion --output text
    - eksctl create addon --cluster $EKS_CLUSTER_NAME --name vpc-cni --version latest --force # Necessary for aws load balancer controller
    - eksctl create addon --name aws-ebs-csi-driver --cluster $EKS_CLUSTER_NAME --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --force # --service-account-role-arn arn:aws:organizations::577409777403:account/o-leoggup8v8/577409777403
    - helm repo add eks https://aws.github.io/eks-charts
    - helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$EKS_CLUSTER_NAME --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller
    - sleep 30 # aws-load-balancer-webhook-service needs time to be up and running, or else next line will fail
    - kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/examples/2048/2048_full.yaml
  tags:
    - build_job_docker
  when: manual

deploy_ai:
  stage: ai
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.2
  script:
    - echo $EKS_CLUSTER_NAME
    - echo $AWS_REGION
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    - helm repo add eks https://aws.github.io/eks-charts
    - helm repo update eks
    - helm repo update
    - curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.37/k8sgpt_amd64.deb
    - dpkg -i k8sgpt_amd64.deb
    - helm repo add k8sgpt https://charts.k8sgpt.ai/
    - helm repo update
    - helm list --all-namespaces
    - kubectl delete crd k8sgpts.core.k8sgpt.ai || true
    - helm uninstall release --namespace k8sgpt-operator-system || true
    - helm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace || true
    - helm repo add go-skynet https://go-skynet.github.io/helm-charts/
    - |  
      cat <<EOF > values.yaml
      replicaCount: 1

      deployment:
      image: quay.io/go-skynet/local-ai:master-ffmpeg-core
      env:
        threads: 4
        context_size: 512
      modelsPath: "/models"
      
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

      # Prompt templates to include
      # Note: the keys of this map will be the names of the prompt template files
      promptTemplates:
        default: |
          The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.
          ### Prompt:
          {{.Input}}
          ### Response:

      # Models to download at runtime
      models:
      # Whether to force download models even if they already exist
      forceDownload: false

      # The list of URLs to download models from
      # Note: the name of the file will be the name of the loaded model
      list:
        - url: "https://gpt4all.io/models/ggml-gpt4all-j.bin"
          # basicAuth: base64EncodedCredentials

      # Persistent storage for models and prompt templates.
      # PVC and HostPath are mutually exclusive. If both are enabled,
      # PVC configuration takes precedence. If neither are enabled, ephemeral
      # storage is used.
      persistence:
        pvc:
          enabled: false
          size: 6Gi
          accessModes:
            - ReadWriteOnce

          annotations: {}

          # Optional
          storageClass: ~

        hostPath:
          enabled: false
          path: "/models"

      service:
      type: ClusterIP
      port: 80
      targetPort: 8080
      annotations: {}
      # If using an AWS load balancer, you'll need to override the default 60s load balancer idle timeout
      # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "1200"

      ingress:
      enabled: false
      className: ""
      annotations:
        {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      hosts:
        - host: chart-example.local
          paths:
            - path: /
              pathType: ImplementationSpecific
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local

      nodeSelector: {}

      tolerations: []

      affinity: {}
      EOF
    # - helm repo add go-skynet https://go-skynet.github.io/helm-charts/
    # - helm install local-ai go-skynet/local-ai -f values.yml
    # - kubectl create -f namespace.yml --dry-run=client -o yaml | kubectl apply -f -
    #- kubectl create -f pvc.yml --dry-run=client -o yaml | kubectl apply -f -
    # - kubectl create -f storageclass.yml --dry-run=client -o yaml | kubectl apply -f -
    #- kubectl create -f deployment.yml --dry-run=client -o yaml | kubectl apply -f -
    #- kubectl create -f service.yml --dry-run=client -o yaml | kubectl apply -f -
    # - kubectl create -f ingress.yml --dry-run=client -o yaml | kubectl apply -f -
    # - kubectl create -f local-ai-test.yml --dry-run=client -o yaml --validate=false | kubectl apply -f -
    # - kubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI/master/examples/kubernetes/deployment.yaml 
    - helm install local-ai go-skynet/local-ai -f values.yaml
    - |
      kubectl apply -f - << EOF
      apiVersion: core.k8sgpt.ai/v1alpha1
      kind: K8sGPT
      metadata:
        name: k8sgpt-local-ai
        namespace: k8sgpt-operator-system
      spec:
        ai:
          enabled: true
          model: ggml-gpt4all-j
          backend: localai
          baseUrl: http://local-ai.local-ai.svc.cluster.local:8080/v1
        noCache: false
        repository: ghcr.io/k8sgpt-ai/k8sgpt
        version: v0.3.8
      EOF
    # - |
    #   kubectl -n local-ai apply -f - << EOF
    #   apiVersion: core.k8sgpt.ai/v1alpha1
    #   kind: K8sGPT
    #   metadata:
    #     name: k8sgpt-local
    #     namespace: local-ai
    #   spec:
    #     backend: localai  
    #     # use the same model name here as the one you plugged
    #     # into the LocalAI helm chart's values.yaml
    #     model: ggml-gpt4all-j.bin
    #     # kubernetes-internal DNS name of the local-ai Service
    #     baseUrl: http://local-ai.local-ai.svc.cluster.local:8080/v1
    #     # allow K8sGPT to store AI analyses in an in-memory cache,
    #     # otherwise your cluster may get throttled :)
    #     noCache: false
    #     version: v0.2.7
    #     enableAI: true
    #   EOF
    # nj,- kubectl apply -f https://raw.githubusercontent.com/mudler/LocalAI/master/examples/kubernetes/deployment.yaml
    # - kubectl get results -o json | jq .
  tags:
    - build_job_docker
  when: manual

monitoring:
  stage: monitor
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.2
  script:
    # - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    # - chmod +x ./kubectl
    # - mv ./kubectl /usr/local/bin/kubectl
    - echo $EKS_CLUSTER_NAME
    - echo $AWS_REGION
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME
    # - curl -L https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 -o /tmp/get_helm.sh
    # - chmod 0700 /tmp/get_helm.sh
    # - /tmp/get_helm.sh
    - helm repo add eks https://aws.github.io/eks-charts
    - helm repo update eks
    - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    - helm repo update
    - kubectl create -f namespace.yml --dry-run=client -o yaml | kubectl apply -f -
    - |
      cat <<EOF > values.yaml
      prometheus:
        prometheusSpec:
          serviceMonitorSelectorNilUsesHelmValues: false
          ruleSelectorNilUsesHelmValues: false
          podMonitorSelectorNilUsesHelmValues: false
          probeSelectorNilUsesHelmValues: false
      grafana:
        enabled: true
        adminPassword: Azurev69007
        service:
          type: ClusterIP
          port: 80
          targetPort: 3000
        ingress:
          enabled: true
          ingressClassName: alb
          annotations:
            alb.ingress.kubernetes.io/scheme: internet-facing
            alb.ingress.kubernetes.io/target-type: ip
          paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: kube-prometheus-stack-grafana
                  port:
                    number: 80
          tls: []
      EOF
    # - helm uninstall kube-prometheus-stack
    - helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n monitor-graf-prom --create-namespace -f values.yaml
  tags:
    - build_job_docker
  when: manual

clean:
  stage: destroy
  image:
    name: registry.gitlab.com/nevii/terraform_eks/awsclitoolbox:v0.2
  environment:
    name: production
  before_script:
      - sleep 5
    # - yum install -y openssl git tar gzip build-essential # needed for Helm chart verification
  script:
    # - curl -L https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 -o /tmp/get_helm.sh
    # - chmod 0700 /tmp/get_helm.sh
    # - /tmp/get_helm.sh # Install Helm
    - echo $EKS_CLUSTER_NAME
    - echo $AWS_REGION
    - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME  # $EKS_CLUSTER_NAME demo-eks-appv1
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region $AWS_REGION
    # - kubectl delete -f 2048_full.yaml
    # - eksctl delete fargateprofile --cluster $EKS_CLUSTER_NAME --name <fargate-profile-name>
    # - eksctl delete nodegroup --cluster $EKS_CLUSTER_NAME --name <node-group-name>
    - eksctl delete cluster --name $EKS_CLUSTER_NAME
  tags:
    - build_job_docker
  when: manual
